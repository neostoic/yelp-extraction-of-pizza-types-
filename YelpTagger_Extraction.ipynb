{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging types of pizza with ddlite: candidate extraction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Here's the pipeline:\n",
    "\n",
    "1. Obtain and parse input data (yelp reviews from pizza restaurants)\n",
    "2. Extract candidates for tagging\n",
    "3. Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cPickle, os, sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from ddlite import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input data\n",
    "The following code parses the json files of the academic dataset getting up to 50 reviews per restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza restaurantIDs count 2223\n",
      "No .DS_Store file\n",
      "44806\n"
     ]
    }
   ],
   "source": [
    "import os, codecs, json, shutil, glob\n",
    "\n",
    "#assign variables to path to the json files\n",
    "FREVIEW = os.path.join('yelp_data', 'yelp_academic_dataset_review.json')\n",
    "FBUSINESS = os.path.join('yelp_data', 'yelp_academic_dataset_business.json')\n",
    "\n",
    "#delete any existing files in reviews folder\n",
    "files = glob.glob('/yelp_pizza_reviews/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "def getReviews(quantOfRest=100000, quantOfReviewsPerRest=5000000):\n",
    "    #get restaurant ids from business ids\n",
    "    restaurantIDs = []\n",
    "    with codecs.open(FBUSINESS,'rU','utf-8') as f:\n",
    "        for business in f:\n",
    "            if \"Restaurants\" in json.loads(business)[\"categories\"]:\n",
    "                if \"Pizza\" in json.loads(business)[\"categories\"]:\n",
    "                    restaurantIDs.append(json.loads(business)['business_id'])\n",
    "    print \"Pizza restaurantIDs count\", len(restaurantIDs)\n",
    "    \n",
    "    #create dictionary of RestaurantID to Reviews\n",
    "    dictRestaurantIDsToReview = {}\n",
    "    with codecs.open(FREVIEW,'rU','utf-8') as f:\n",
    "        for review in f:\n",
    "            reviewText = json.loads(review)['text']\n",
    "            ID = json.loads(review)['business_id']\n",
    "            if ID in restaurantIDs:\n",
    "                if ID in dictRestaurantIDsToReview.keys():\n",
    "                    if len(dictRestaurantIDsToReview.get(ID)) < quantOfReviewsPerRest:\n",
    "                        dictRestaurantIDsToReview.get(ID).append(reviewText)\n",
    "                else:\n",
    "                    if len(dictRestaurantIDsToReview.keys()) < quantOfRest:\n",
    "                        dictRestaurantIDsToReview[ID] = [reviewText]\n",
    "                    else:\n",
    "                        break\n",
    "    return dictRestaurantIDsToReview\n",
    "\n",
    "#get reviews in the form of a dictionary\n",
    "dictRestaurantIDsToReview = getReviews(quantOfReviewsPerRest=50)\n",
    "\n",
    "#save reviews to folder as text files.  Each restaurant has separate review file.\n",
    "count = 0\n",
    "for restID in dictRestaurantIDsToReview.keys():\n",
    "    reviews = \"\"\n",
    "    for review in dictRestaurantIDsToReview[restID]:\n",
    "        review = review.encode('ascii', errors='ignore') + \" \"\n",
    "        count += 1\n",
    "        reviews += review\n",
    "    open(\"yelp_pizza_reviews/reviews_\" + restID + \".txt\", \"w+\").write(reviews)\n",
    "\n",
    "#try to remove .DS_Store file.  Otherwise DocParser throws an exception\n",
    "try:\n",
    "    os.remove(\"yelp_pizza_reviews/.DS_Store\")\n",
    "except:\n",
    "    print \"No .DS_Store file\"\n",
    "    \n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = DocParser('yelp_pizza_reviews/')\n",
    "docs = list(dp.readDocs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use CoreNLP via ddlite's `SentenceParser` to parse each sentence. `DocParser` can handle this too; we didn't really need that call above. This can take a little while, so if the example has already been run, we'll reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence(words=[u'This', u'restaurant', u'used', u'to', u'be', u'called', u'La', u'Piazza', u'.'], lemmas=[u'this', u'restaurant', u'use', u'to', u'be', u'call', u'La', u'Piazza', u'.'], poses=[u'DT', u'NN', u'VBN', u'TO', u'VB', u'VBN', u'NNP', u'NNP', u'.'], dep_parents=[2, 0, 2, 6, 6, 3, 8, 6, 2], dep_labels=[u'det', u'ROOT', u'acl', u'mark', u'auxpass', u'xcomp', u'compound', u'xcomp', u'punct'], sent_id=0, doc_id=0, text=u'This restaurant used to be called La Piazza.', token_idxs=[0, 5, 16, 21, 24, 27, 34, 37, 43], doc_name='reviews_CwKyfU1JQRd3rHSYORG3hw.txt')\n"
     ]
    }
   ],
   "source": [
    "docs = None\n",
    "\n",
    "pkl_f = 'yelp_tag_saved_sents_v4.pkl'\n",
    "try:\n",
    "    with open(pkl_f, 'rb') as f:\n",
    "        sents = cPickle.load(f)\n",
    "except:\n",
    "    %time sents = dp.parseDocSentences()\n",
    "    with open(pkl_f, 'w+') as f:\n",
    "        cPickle.dump(sents, f)\n",
    "\n",
    "print sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting candidates with matchers\n",
    "We use regex matchers to extract candidates. The dictionary match should provide fairly high recall.  For each topping, we generate a regex matchers that requires the topping be the first word and pizza be the last word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toppings = [\"mushroom\",\"pepperoni\",\"sausage\",\"hawaiian\",\"pineapple\",\"beef\",\"pork\",\"chicken\",\n",
    "            \"Italian\",\"salami\",\"meatball\",\"ham\",\"bacon\",\"spinach\",\"tomato\",\"onion\",\"pepper\"]\n",
    "\n",
    "def gen_regex_match(topping):\n",
    "    pattern = topping + r\"\\s\\w+\\spizza\"\n",
    "    m1 = RegexNgramMatch(label=topping+\"m1\", regex_pattern=pattern, ignore_case=True)\n",
    "    pattern = topping + r\"\\s\\w+\\s\\w+\\spizza\"\n",
    "    m2 = RegexNgramMatch(label=topping+\"m2\", regex_pattern=pattern, ignore_case=True)\n",
    "    pattern = topping + r\"\\s\\w+\\s\\w+\\s\\w+\\spizza\"\n",
    "    m3 = RegexNgramMatch(label=topping+\"m3\", regex_pattern=pattern, ignore_case=True)\n",
    "    return [m1, m2, m3]\n",
    "\n",
    "args = []\n",
    "for topping in toppings:\n",
    "    args += gen_regex_match(topping)\n",
    "\n",
    "    \n",
    "# old rules\n",
    "#pizza_regex1 = RegexNgramMatch(label='Pizza', regex_pattern=r'\\w+\\spizza', ignore_case=True)\n",
    "#pizza_regex2 = RegexNgramMatch(label='Pizza', regex_pattern=r'\\w+\\s\\w+\\spizza', ignore_case=True)\n",
    "#pizza_regex3 = RegexNgramMatch(label='Pizza', regex_pattern=r'\\w+\\s\\w+\\s\\w+\\spizza', ignore_case=True)\n",
    "#pizza_regex4 = RegexNgramMatch(label='Pizza', regex_pattern=r'\\w+\\s\\w+\\s\\w+\\s\\w+\\spizza', ignore_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine all matchers\n",
    "CE = Union(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the candidates\n",
    "We'll use our unioned candidate extractor to extract our candidate entities from the sentences into an `Entities` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "E = Entities(sents, CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1021"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of entities we extracted\n",
    "len(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parse tree visualization of the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".node {\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       ".node circle {\n",
       "  fill: #fff;\n",
       "  stroke: steelblue;\n",
       "  stroke-width: 3px;\n",
       "}\n",
       "\n",
       ".node text {\n",
       "  font: 12px sans-serif;\n",
       "}\n",
       "\n",
       ".edge {\n",
       "  fill: none;\n",
       "  stroke: #ccc;\n",
       "  stroke-width: 2px;\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       ".highlight {\n",
       "  stroke: red;\n",
       "  stroke-width: 3px;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<!--Provide the canvas id (twice) and the words via python string formatting here--!>\n",
       "<div id=\"tree-chart-5285751923761835007\"></div>\n",
       "<div id=\"raw-seq-5285751923761835007\">\n",
       "<span class=\"word-5285751923761835007-0\">Grandma</span> <span class=\"word-5285751923761835007-1\">G</span> <span class=\"word-5285751923761835007-2\">'s</span> <span class=\"word-5285751923761835007-3\">Rosemary</span> <span class=\"word-5285751923761835007-4\">Chicken</span> <span class=\"word-5285751923761835007-5\">Potato</span> <span class=\"word-5285751923761835007-6\">Pizza</span> <span class=\"word-5285751923761835007-7\">is</span> <span class=\"word-5285751923761835007-8\">THE</span> <span class=\"word-5285751923761835007-9\">best</span> <span class=\"word-5285751923761835007-10\">and</span> <span class=\"word-5285751923761835007-11\">the</span> <span class=\"word-5285751923761835007-12\">Pasta</span> <span class=\"word-5285751923761835007-13\">Basta</span> <span class=\"word-5285751923761835007-14\">rocks</span> <span class=\"word-5285751923761835007-15\">,</span> <span class=\"word-5285751923761835007-16\">too</span> <span class=\"word-5285751923761835007-17\">.</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "$.getScript(\"http://d3js.org/d3.v3.min.js\", function () {\n",
       "// See http://bl.ocks.org/d3noob/8375092\n",
       "// Three vars need to be provided via python string formatting:\n",
       "var chartId = \"5285751923761835007\";\n",
       "var root = {\"attrib\": {\"token_idx\": \"1633\", \"word\": \"best\", \"dep_label\": \"ROOT\", \"pos\": \"JJS\", \"lemma\": \"best\", \"word_idx\": \"9\", \"dep_parent\": \"0\"}, \"children\": [{\"attrib\": {\"token_idx\": \"1620\", \"word\": \"Pizza\", \"dep_label\": \"nsubj\", \"pos\": \"NNP\", \"lemma\": \"Pizza\", \"word_idx\": \"6\", \"dep_parent\": \"10\"}, \"children\": [{\"attrib\": {\"token_idx\": \"1605\", \"word\": \"Chicken\", \"dep_label\": \"compound\", \"pos\": \"NNP\", \"lemma\": \"Chicken\", \"word_idx\": \"4\", \"dep_parent\": \"7\"}, \"children\": [{\"attrib\": {\"token_idx\": \"1592\", \"word\": \"G\", \"dep_label\": \"nmod:poss\", \"pos\": \"NNP\", \"lemma\": \"G\", \"word_idx\": \"1\", \"dep_parent\": \"5\"}, \"children\": [{\"attrib\": {\"token_idx\": \"1584\", \"word\": \"Grandma\", \"dep_label\": \"compound\", \"pos\": \"NNP\", \"lemma\": \"Grandma\", \"word_idx\": \"0\", \"dep_parent\": \"2\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"1593\", \"word\": \"'s\", \"dep_label\": \"case\", \"pos\": \"POS\", \"lemma\": \"'s\", \"word_idx\": \"2\", \"dep_parent\": \"2\"}, \"children\": []}]}, {\"attrib\": {\"token_idx\": \"1596\", \"word\": \"Rosemary\", \"dep_label\": \"compound\", \"pos\": \"NNP\", \"lemma\": \"Rosemary\", \"word_idx\": \"3\", \"dep_parent\": \"5\"}, \"children\": []}]}, {\"attrib\": {\"token_idx\": \"1613\", \"word\": \"Potato\", \"dep_label\": \"compound\", \"pos\": \"NNP\", \"lemma\": \"Potato\", \"word_idx\": \"5\", \"dep_parent\": \"7\"}, \"children\": []}]}, {\"attrib\": {\"token_idx\": \"1626\", \"word\": \"is\", \"dep_label\": \"cop\", \"pos\": \"VBZ\", \"lemma\": \"be\", \"word_idx\": \"7\", \"dep_parent\": \"10\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"1629\", \"word\": \"THE\", \"dep_label\": \"det\", \"pos\": \"DT\", \"lemma\": \"the\", \"word_idx\": \"8\", \"dep_parent\": \"10\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"1638\", \"word\": \"and\", \"dep_label\": \"cc\", \"pos\": \"CC\", \"lemma\": \"and\", \"word_idx\": \"10\", \"dep_parent\": \"10\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"1658\", \"word\": \"rocks\", \"dep_label\": \"conj\", \"pos\": \"NNS\", \"lemma\": \"rock\", \"word_idx\": \"14\", \"dep_parent\": \"10\"}, \"children\": [{\"attrib\": {\"token_idx\": \"1642\", \"word\": \"the\", \"dep_label\": \"det\", \"pos\": \"DT\", \"lemma\": \"the\", \"word_idx\": \"11\", \"dep_parent\": \"15\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"1646\", \"word\": \"Pasta\", \"dep_label\": \"compound\", \"pos\": \"NNP\", \"lemma\": \"Pasta\", \"word_idx\": \"12\", \"dep_parent\": \"15\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"1652\", \"word\": \"Basta\", \"dep_label\": \"compound\", \"pos\": \"NNP\", \"lemma\": \"Basta\", \"word_idx\": \"13\", \"dep_parent\": \"15\"}, \"children\": []}]}, {\"attrib\": {\"token_idx\": \"1663\", \"word\": \",\", \"dep_label\": \"punct\", \"pos\": \",\", \"lemma\": \",\", \"word_idx\": \"15\", \"dep_parent\": \"10\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"1665\", \"word\": \"too\", \"dep_label\": \"advmod\", \"pos\": \"RB\", \"lemma\": \"too\", \"word_idx\": \"16\", \"dep_parent\": \"10\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"1668\", \"word\": \".\", \"dep_label\": \"punct\", \"pos\": \".\", \"lemma\": \".\", \"word_idx\": \"17\", \"dep_parent\": \"10\"}, \"children\": []}]};\n",
       "var highlightIdxs = [[4, 5, 6]];\n",
       "\n",
       "// Highlight words / nodes\n",
       "var COLORS = [\"#ff5c33\", \"#ffcc00\", \"#33cc33\", \"#3399ff\"];\n",
       "function highlightWords() {\n",
       "  for (var i=0; i < highlightIdxs.length; i++) {\n",
       "    var c = COLORS[i];\n",
       "    var idxs = highlightIdxs[i];\n",
       "    for (var j=0; j < idxs.length; j++) {\n",
       "      d3.selectAll(\".word-\"+chartId+\"-\"+idxs[j]).style(\"stroke\", c).style(\"background\", c);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "// Constants\n",
       "var margin = {top: 20, right: 20, bottom: 20, left: 20},\n",
       "width = 800 - margin.left - margin.right,\n",
       "height = 350 - margin.top - margin.bottom,\n",
       "R = 5;\n",
       "\n",
       "// Create the d3 tree object\n",
       "var tree = d3.layout.tree()\n",
       "  .size([width, height]);\n",
       "\n",
       "// Create the svg canvas\n",
       "var svg = d3.select(\"#tree-chart-\" + chartId)\n",
       "  .append(\"svg\")\n",
       "  .attr(\"width\", width + margin.left + margin.right)\n",
       "  .attr(\"height\", height + margin.top + margin.bottom)\n",
       "  .append(\"g\")\n",
       "  .attr(\"transform\", \"translate(\" + margin.left + \",\" + margin.top + \")\");\n",
       "\n",
       "function renderTree() {\n",
       "  var nodes = tree.nodes(root),\n",
       "  edges = tree.links(nodes);\n",
       "\n",
       "  // Place the nodes\n",
       "  var nodeGroups = svg.selectAll(\"g.node\")\n",
       "    .data(nodes)\n",
       "    .enter().append(\"g\")\n",
       "    .attr(\"class\", \"node\")\n",
       "    .attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; });\n",
       "       \n",
       "  // Append circles\n",
       "  nodeGroups.append(\"circle\")\n",
       "    //.on(\"click\", function() {\n",
       "    //  d3.select(this).classed(\"highlight\", !d3.select(this).classed(\"highlight\")); })\n",
       "    .attr(\"r\", R)\n",
       "    .attr(\"class\", function(d) { return \"word-\"+chartId+\"-\"+d.attrib.word_idx; });\n",
       "     \n",
       "  // Append the actual word\n",
       "  nodeGroups.append(\"text\")\n",
       "    .text(function(d) { return d.attrib.word; })\n",
       "    .attr(\"text-anchor\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? \"start\" : \"middle\"; })\n",
       "    .attr(\"dx\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? R + 3 : 0; })\n",
       "    .attr(\"dy\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? 0 : 3*R + 3; });\n",
       "\n",
       "  // Place the edges\n",
       "  var edgePaths = svg.selectAll(\"path\")\n",
       "    .data(edges)\n",
       "    .enter().append(\"path\")\n",
       "    .attr(\"class\", \"edge\")\n",
       "    .on(\"click\", function() {\n",
       "      d3.select(this).classed(\"highlight\", !d3.select(this).classed(\"highlight\")); })\n",
       "    .attr(\"d\", d3.svg.diagonal());\n",
       "}\n",
       "\n",
       "renderTree();\n",
       "highlightWords();\n",
       "});\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E[0].render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Chicken', u'and', u'a', u'Hawaiian', u'pizza']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E[1].mention(attribute='words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll pickle the extracted candidates from our `Entities` object for use in learning ipython module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "E.dump_candidates('yelp_tag_saved_entities_v5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
